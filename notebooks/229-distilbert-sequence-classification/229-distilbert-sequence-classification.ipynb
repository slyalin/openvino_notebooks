{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2b748f3",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with OpenVINO™\n",
    "\n",
    "**Sentiment analysis** is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. This notebook demonstrates how to convert and run a sequence classification model using OpenVINO. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc41ac0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, AutoTokenizer\n",
    "import openvino.runtime as ov\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36add5c2",
   "metadata": {},
   "source": [
    "## Initializing the Model\n",
    "We will use the transformer-based [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db803ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70bbf5",
   "metadata": {},
   "source": [
    "## Initializing the Tokenizer\n",
    "\n",
    "Text Preprocessing cleans the text-based input data so it can be fed into the model. [Tokenization](https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4) splits paragraphs and sentences into smaller units that can be more easily assigned meaning. It involves cleaning the data and assigning tokens or IDs to the words, so they are represented in a vector space where similar words have similar vectors. This helps the model understand the context of a sentence. Here, we will use [AutoTokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer) - a pre-trained tokenizer from Hugging Face: ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782bbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=checkpoint, model_max_length=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b00e300",
   "metadata": {},
   "source": [
    "## Convert to OpenVINO\n",
    "\n",
    "Convert both tokenizer and the model to a single OpenVINO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools.mo import convert_model\n",
    "from convert_tokenizer import convert_tokenizer\n",
    "from convert_tokenizer import connect_models\n",
    "\n",
    "# Convert the tokenizer to OpenVINO model\n",
    "ov_tokenizer = convert_tokenizer(tokenizer)\n",
    "\n",
    "# A sample input is required for tracing inside convert_model(model)\n",
    "example_tokens = tokenizer('example text', return_tensors='pt')\n",
    "ov_model = convert_model(model, example_input={**example_tokens})\n",
    "\n",
    "# Now connect tokenizer and the main model together\n",
    "ov_combined_model = connect_models(ov_tokenizer, ov_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc074e",
   "metadata": {},
   "source": [
    "OpenVINO™ Runtime uses the [Infer Request](https://docs.openvino.ai/2023.0/openvino_docs_OV_UG_Infer_request.html) mechanism which enables running models on different devices in asynchronous or synchronous manners. The model graph is sent as an argument to the OpenVINO API and an inference request is created. The default inference mode is AUTO but it can be changed according to requirements and hardware available. You can explore the different inference modes and their usage [in documentation.](https://docs.openvino.ai/2023.0/openvino_docs_Runtime_Inference_Modes_Overview.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a2644",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "core = ov.Core()\n",
    "compiled_model = core.compile_model(ov_combined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de01fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Defining a softmax function to extract\n",
    "    the prediction from the output of the IR format\n",
    "    Parameters: Logits array\n",
    "    Returns: Probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e778507",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from str_pack import pack_strings   # FIXME: required if string tensors are not supported\n",
    "\n",
    "def infer(input_text):\n",
    "    \"\"\"\n",
    "    Creating a generic inference function\n",
    "    to read the batch of input strings and infer the result\n",
    "    into 2 classes: Positive or Negative.\n",
    "    Parameters: Text to be processed (list of strings)\n",
    "    Returns: A list of labels, one per each input string: Positive or Negative.\n",
    "    \"\"\"\n",
    "\n",
    "    label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "    result = compiled_model(pack_strings(input_text))\n",
    "    return [label[np.argmax(softmax(x))] for x in result[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e79fd",
   "metadata": {},
   "source": [
    "### For a single input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf976f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"I had a wonderful day\"\n",
    "start_time = time.perf_counter()\n",
    "result = infer([input_text])\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "print(\"Label: \", result[0])\n",
    "print(\"Total Time: \", \"%.2f\" % total_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4d013",
   "metadata": {},
   "source": [
    "### Read from a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f57d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "with open(\"../data/text/food_reviews.txt\", \"r\") as f:\n",
    "    input_text = f.read().splitlines()\n",
    "    results = infer(input_text) # infer entire batch in one call\n",
    "    for line, result in zip(input_text, results):\n",
    "        print(\"User Input: \", line)\n",
    "        print(\"Label: \", result, \"\\n\")\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "print(\"Total Time: \", \"%.2f\" % total_time, \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
